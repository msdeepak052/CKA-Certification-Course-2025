
## Task

This task must be completed on **node01**. Use the following credentials to access the node via SSH:

* **Username:** `bob`
* **Password:** `caleston123`

### Objective

Prepare **node01** for a Kubernetes installation by setting up a container runtime.

1. Install the `cri-docker_0.3.16.3-0.debian.deb` package located in `/root`.
2. Ensure the `cri-docker` service is **running** and **enabled** to start on boot.

---

## Solution

### 1. Access the Node

SSH into **node01** using the provided password:

```bash
ssh bob@node01

```

### 2. Elevate Privileges

Switch to the root user to execute administrative commands:

```bash
sudo -i

```

### 3. Install the Package

Use `dpkg` to install the `.deb` package located in the `/root` directory:

```bash
dpkg -i /root/cri-docker_0.3.16.3-0.debian.deb

```

### 4. Manage the Service

Start the `cri-docker` service and configure it to launch automatically at boot:

```bash
systemctl start cri-docker
systemctl enable cri-docker

```

### 5. Verification

Confirm the service is active and properly enabled:

| Command | Expected Output |
| --- | --- |
| `systemctl is-active cri-docker` | `active` |
| `systemctl is-enabled cri-docker` | `enabled` |

---

Here is the formatted Markdown for the troubleshooting task.

---

## Task

A new application, **orange**, has been deployed, but it is failing to start correctly. Identify the root cause and resolve the issue.

---

## Solution

### 1. Identify the Issue

Inspect the details of the **orange** pod to find the error:

```bash
kubectl describe po orange

```

Check the `initContainers` section. You will notice a syntax error in the command:

> **Command:**
> `sh -c sleeeep 2;`

The command `sleeeep` is misspelled and is causing the container to fail during initialization.

### 2. Update the Pod

Since you cannot edit most fields of a running pod directly (including the command of an initContainer), use the `edit` command:

```bash
kubectl edit po orange

```

1. Locate the `initContainers` section in the YAML manifest.
2. Correct `sleeeep 2;` to `sleep 2;`.
3. Save and exit the editor.

### 3. Apply the Changes

Because Kubernetes does not allow live updates to this specific field, saving the file will generate a temporary manifest in your `/tmp/` directory. Use that file to replace the existing pod:

```bash
kubectl replace -f /tmp/kubectl-edit-xxxx.yaml --force

```

*(Note: Replace `xxxx` with the actual random string generated by the edit command.)*

**The `--force` flag ensures the old pod is deleted and recreated immediately with the corrected configuration.**

---

### 4. Verification

Check the status to ensure the pod is now running:

```bash
kubectl get po orange

```

---


## Task 11: Deploy a Vertical Pod Autoscaler (VPA)

### Objective

Deploy a VPA named `analytics-vpa` for the `analytics-deployment` in the `default` namespace. The VPA must optimize CPU and memory requests using the **Recreate** update mode.

### Solution

Execute the following command to create the VPA resource using a "here document" (EOF):

```yaml
kubectl create -n default -f - <<EOF
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Recreate"
EOF

```

---

## Task 12: Create a Kubernetes Gateway Resource

### Objective

Create a Gateway resource with the following specifications:

* **Name:** `web-gateway`
* **Namespace:** `nginx-gateway`
* **Gateway Class:** `nginx`
* **Listener:** Port `80`, Protocol `HTTP`, Name `http`

### Solution

Run the following command to deploy the Gateway:

```yaml
kubectl create -n nginx-gateway -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      protocol: HTTP
      port: 80
EOF

```

---

## Task 13: Update and Upgrade a Helm Release

### Objective

Update the Helm repository and upgrade the existing `kk-mock1` release in the `kk-ns` namespace to version `18.1.15`.

### Solution

#### 1. Locate the Release

List all Helm releases across all namespaces to confirm the deployment:

```bash
helm ls -A

```

#### 2. Update the Repository

Update your local Helm chart cache to fetch the latest available versions:

```bash
helm repo update

```

#### 3. Verify Available Versions

Search for the specific chart version to ensure `18.1.15` is available:

```bash
helm search repo kk-mock1/nginx -n kk-ns -l | head -n 30

```

#### 4. Upgrade the Release

Perform the upgrade to the target version. If you need to scale the application simultaneously, you can append the replica flag:

```bash
helm upgrade kk-mock1 kk-mock1/nginx -n kk-ns --version 18.1.15

```

#### 5. Verify the Upgrade

Check the status and version of the release:

```bash
helm ls -n kk-ns

```

> **Note:** Verify the version number under the **CHART** column in the output.

---

# Kubernetes Administration Tasks Documentation

---

## Q. 1

### Task

You are an administrator preparing your environment to deploy a Kubernetes cluster using kubeadm. Adjust the following network parameters on the system to the following values, and make sure your changes persist reboots:

```
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
```

---

### Solution

Use sysctl to adjust system parameters and make sure they persist across reboots.

To set the required sysctl parameters and make them persistent:

```bash
echo 'net.ipv4.ip_forward = 1' >> /etc/sysctl.conf
echo 'net.bridge.bridge-nf-call-iptables = 1' >> /etc/sysctl.conf
sysctl -p
```

To verify:

```bash
sysctl net.ipv4.ip_forward
sysctl net.bridge.bridge-nf-call-iptables
```

---

### Details

---

## Q. 2

### Task

Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace.

---

### Solution

Pods authenticate to the API Server using ServiceAccounts. If the serviceAccount name is not specified, the default service account for the namespace is used during a pod creation.

Reference:
[https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/)

Now, create a service account pvviewer:

```bash
kubectl create serviceaccount pvviewer
```

To create a clusterrole:

```bash
kubectl create clusterrole pvviewer-role --resource=persistentvolumes --verb=list
```

To create a clusterrolebinding:

```bash
kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer
```

Solution manifest file to create a new pod called pvviewer as follows:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: pvviewer
  name: pvviewer
spec:
  containers:
  - image: redis
    name: pvviewer
  # Add service account name
  serviceAccountName: pvviewer
```

---

### Details

---

## Q. 3

### Task

Create a StorageClass named rancher-sc with the following specifications:

The provisioner should be rancher.io/local-path.
The volume binding mode should be WaitForFirstConsumer.
Volume expansion should be enabled.

---

### Solution

Use kubectl create storageclass and specify the required provisioner and parameters.

Create a manifest file rancher-sc.yaml:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rancher-sc
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

Apply the file on the cluster:

```bash
kubectl apply -f rancher-sc.yaml
```

To verify:

```bash
kubectl get storageclass rancher-sc -o yaml
```

---

### Details

---

## Q. 4

### Task

Create a ConfigMap named app-config in the namespace cm-namespace with the following key-value pairs:

ENV=production
LOG_LEVEL=info

Then, modify the existing Deployment named cm-webapp in the same namespace to use the app-config ConfigMap by setting the environment variables ENV and LOG_LEVEL in the container from the ConfigMap.

---

### Solution

Use kubectl create configmap to create the ConfigMap and kubectl set env to modify the deployment to use the ConfigMap.

Create the ConfigMap:

```bash
kubectl create configmap app-config -n cm-namespace \
  --from-literal=ENV=production \
  --from-literal=LOG_LEVEL=info
```

Patch the deployment to use the config:

```bash
kubectl set env deployment/cm-webapp -n cm-namespace \
  --from=configmap/app-config
```

---

### Details

---

## Q. 5

### Task

Create a PriorityClass named low-priority with a value of 50000. A pod named lp-pod exists in the namespace low-priority. Modify the pod to use the priority class you created. Recreate the pod if necessary.

---

### Solution

Use kubectl apply to create the PriorityClass, then edit the existing pod to add the priority class.

Create the priority class manifest file pc.yaml

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 50000
globalDefault: false
description: "Low priority class"
```

Apply the manifest file

```bash
kubectl apply -f pc.yaml
```

Inspect the pod definition to be able to recreate it:

```bash
kubectl get pod lp-pod -n low-priority -o yaml
```

Create and update the new yaml lp-pod.yaml for the pod after adding the priority class:

```bash
# vi lp-pod.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lp-pod
  namespace: low-priority
spec:
  priorityClassName: low-priority
  containers:
  - name: nginx
    image: nginx
```

Delete and recreate the pod using the configured priority class

```bash
kubectl delete pod lp-pod -n low-priority
kubectl apply -f lp-pod.yaml
```

---

### Details

---

## Q. 6

### Task

---

## Q. 7

### Task

---

## Q. 8

### Task

---

## Q. 9

### Task

A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it.

---

### Solution

Verify host and port for kube-apiserver are correct.

Open the super.kubeconfig in vi editor.

Change the 9999 port to 6443 and run the below command to verify:

```bash
kubectl cluster-info --kubeconfig=/root/CKA/super.kubeconfig
```

---

### Details

---

## Q. 10

### Task

We have created a new deployment called nginx-deploy. Scale the deployment to 3 replicas. Has the number of replicas increased? Troubleshoot and fix the issue.

---

### Solution

Use the command kubectl scale to increase the replica count to 3.

```bash
kubectl scale deploy nginx-deploy --replicas=3
```

The controller-manager is responsible for scaling up pods of a replicaset. If you inspect the control plane components in the kube-system namespace, you will see that the controller-manager is not running.

```bash
kubectl get pods -n kube-system
```

The command running inside the controller-manager pod is incorrect.
After fix all the values in the file and wait for controller-manager pod to restart.

Alternatively, you can run sed command to change all values at once:

```bash
sed -i 's/kube-contro1ler-manager/kube-controller-manager/g' /etc/kubernetes/manifests/kube-controller-manager.yaml
```

This will fix the issues in controller-manager yaml file.

At last, inspect the deployment by using below command, you should see 3/3 under READY if the fix above was properly performed. Example:

```bash
controlplane ~ âžœ  kubectl get deploy
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deploy   3/3     3            3           6m2s
```

---

### Details

---

## Q. 11

### Task

Create a Horizontal Pod Autoscaler (HPA) api-hpa for the deployment named api-deployment located in the api namespace.
The HPA should scale the deployment based on a custom metric named requests_per_second, targeting an average value of 1000 requests per second across all pods.
Set the minimum number of replicas to 1 and the maximum to 20.

Note: Deployment named api-deployment is available in api namespace. Ignore errors due to the metric requests_per_second not being tracked in metrics-server.

---

### Solution

Under /root/ folder you will find a yaml file api-hpa.yaml. Update the yaml file as per task given.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-deployment
  minReplicas: 1
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
```

Use below command:

```bash
kubectl create -f api-hpa.yaml
```

---

### Details

---

## Q. 12

### Task

Configure the web-route to split traffic between web-service and web-service-v2. The configuration should ensure that 80% of the traffic is routed to web-service and 20% is routed to web-service-v2.

Note: web-gateway, web-service, and web-service-v2 have already been created and are available on the cluster.

---

### Solution

Copy the below YAML file to the terminal and create a HTTP Route.

```bash
kubectl create -n default -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: web-route
  namespace: default
spec:
  parentRefs:
    - name: web-gateway
      namespace: default
  rules:
    - matches:
        - path:
            type: PathPrefix
            value: /
      backendRefs:
        - name: web-service
          port: 80
          weight: 80
        - name: web-service-v2
          port: 80
          weight: 20
EOF
```

---

### Details

---

## Q. 13

### Task

One application, webpage-server-01, is currently deployed on the Kubernetes cluster using Helm. A new version of the application is available in a Helm chart located at /root/new-version.

Validate this new Helm chart, then install it as a new release named webpage-server-02. After confirming the new release is installed, uninstall the old release webpage-server-01.

---

### Solution

In this task, we will use the helm commands. Here are the steps:

Use the helm ls command to list the Helm releases in the default namespace.

```bash
helm ls -n default
```

Validate the Helm chart using the helm lint command:

```bash
cd /root/
helm lint ./new-version
```

Install the new version of the application as a new release named webpage-server-02:

```bash
helm install webpage-server-02 ./new-version
```

Uninstall the old release webpage-server-01 using the following command:

```bash
helm uninstall webpage-server-01 -n default
```

---

### Details

---

## Q. 14

### Task

While preparing to install a CNI plugin on your Kubernetes cluster, you typically need to confirm the cluster-wide Pod network CIDR. Identify the Pod subnet configured for the cluster (the value specified under podSubnet in the kubeadm configuration). Output this CIDR in the format x.x.x.x/x to a file located at /root/pod-cidr.txt.

---

### Solution

To identify the cluster-wide Pod subnet, inspect the kubeadm-config ConfigMap, which contains the ClusterConfiguration used during kubeadm init:

```bash
kubectl -n kube-system get configmap kubeadm-config -o yaml | grep podSubnet
# podSubnet: 172.17.0.0/16
```

Save just the CIDR value to /root/pod-cidr.txt:

```bash
kubectl -n kube-system get configmap kubeadm-config -o yaml \
  | awk '/podSubnet:/{print $2}' > /root/pod-cidr.txt
```

Verify:

```bash
cat /root/pod-cidr.txt
# 172.17.0.0/16
```

---

### Note

Be careful not to confuse Cluster PodCIDR vs Node PodCIDR:

Cluster = Entire pool
Cluster PodCIDR (big range: 172.17.0.0/16)
Find it with :

```bash
kubectl get cm kubeadm-config -n kube-system -o yaml
```

Node = Single slice
Node PodCIDR (small slice: 172.17.0.0/24)
Find it with :

```bash
kubectl get node <name> -o jsonpath='{.spec.podCIDR}'
```



